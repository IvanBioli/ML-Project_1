{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 \n",
    "#### by Fabio, Ivan and Olivier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "# Loading the training data\n",
    "y, tX, ids = load_csv_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undefined values visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of inputs with at least one undefined feature:  72.7544\n"
     ]
    }
   ],
   "source": [
    "#Counting the number of undefined values and the percentage of undefined values\n",
    "counter = np.sum((tX == -999), axis = 0)\n",
    "undef_ind = np.nonzero(counter)[0]\n",
    "plt.bar(np.arange(len(counter)), counter)\n",
    "plt.title(\"Number of undefined values per feature\") \n",
    "plt.show()\n",
    "\n",
    "#Percentage of undefined values\n",
    "N = tX.shape[0]\n",
    "rel_counter = counter / N * 100\n",
    "plt.bar(np.arange(len(rel_counter)), rel_counter)\n",
    "plt.title(\"Percentage of undefined values per feature\") \n",
    "plt.show()\n",
    "\n",
    "#Counting the percentage of inputs with at least one undefined feature \n",
    "counter = np.sum((tX == -999), axis = 1)\n",
    "tot = np.count_nonzero(counter)\n",
    "print(\"Percentage of inputs with at least one undefined feature: \", tot / N * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Same number of undefined features for some features. This are structurally missing features.\n",
    "* Very high percentage for some particular features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and validation sets\n",
    "ratio = 0.8  # Ratio of samples to use in new train set\n",
    "cut_off_point = int(ratio*len(y))\n",
    "np.random.seed(0)  # Fixing a seed for reproducibility\n",
    "rand_ind = np.random.permutation(np.arange(len(y)))\n",
    "y_train, y_valid = y[cut_off_point:], y[:cut_off_point]\n",
    "tX_train, tX_valid = tX[cut_off_point:], tX[:cut_off_point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing undefined entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing undefined entrance:\n",
    "tX_train_nan = np.where(tX_train != -999, tX_train, np.nan)\n",
    "tX_valid_nan = np.where(tX_valid != -999, tX_valid, np.nan)\n",
    "\n",
    "#1. With zero entries\n",
    "tX_train_zeros = np.where(~np.isnan(tX_train_nan), tX_train_nan, 0)\n",
    "tX_valid_zeros = np.where(~np.isnan(tX_valid_nan), tX_valid_nan, 0) \n",
    "tX_train_zeros = standardize(tX_train_zeros)\n",
    "tX_valid_zeros = standardize(tX_valid_zeros)\n",
    "\n",
    "\n",
    "#2. With mean\n",
    "mu_train = np.nanmean(tX_train_nan, axis = 0)\n",
    "tX_train_mean = np.where(~np.isnan(tX_train_nan), tX_train_nan, mu_train)\n",
    "tX_train_mean = standardize(tX_train_mean)\n",
    "mu_valid = np.nanmean(tX_valid_nan, axis = 0)\n",
    "tX_valid_mean = np.where(~np.isnan(tX_valid_nan), tX_valid_nan, mu_valid)\n",
    "tX_valid_mean = standardize(tX_valid_mean)\n",
    "\n",
    "#3. With median\n",
    "mu_train = np.nanmedian(tX_train_nan, axis = 0)\n",
    "tX_train_median = np.where(~np.isnan(tX_train_nan), tX_train_nan, mu_train)\n",
    "tX_train_median = standardize(tX_train_median)\n",
    "mu_valid = np.nanmedian(tX_valid_nan, axis = 0)\n",
    "tX_valid_median = np.where(~np.isnan(tX_valid_nan), tX_valid_nan, mu_valid)\n",
    "tX_valid_median = standardize(tX_valid_median)\n",
    "\n",
    "#4. Adding a binary variable\n",
    "tX_train_binary = np.hstack((tX_train, (2*np.isnan(tX_train_nan[:, undef_ind]) - 1)))\n",
    "tX_train_binary = standardize(tX_train_binary)\n",
    "tX_valid_binary = np.hstack((tX_valid, (2*np.isnan(tX_valid_nan[:, undef_ind]) - 1)))\n",
    "tX_valid_binary = standardize(tX_valid_binary)\n",
    "\n",
    "#5. Removing features with a percentage of undefined inputs above the threshold\n",
    "threshold = 0.7\n",
    "counter = np.sum((tX == -999), axis = 0) / N\n",
    "keep = np.argwhere(counter < threshold).flatten()\n",
    "tX_train_del = tX_train[:, keep]\n",
    "tX_train_del = standardize(tX_train_del)\n",
    "tX_valid_del = tX_valid[:, keep]\n",
    "tX_valid_del = standardize(tX_valid_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "tX_train = standardize(tX_train)\n",
    "tX_valid = standardize(tX_valid)\n",
    "\n",
    "# Creating a dictionary of traning and validation sets\n",
    "train_dict = {'normal': tX_train, 'zeros': tX_train_zeros, 'mean': tX_train_mean, 'median': tX_train_median, 'binary': tX_train_binary, 'deleted': tX_train_del}\n",
    "valid_dict = {'normal': tX_valid, 'zeros': tX_valid_zeros, 'mean': tX_valid_mean, 'median': tX_valid_median, 'binary': tX_valid_binary, 'deleted': tX_valid_del}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_valid, tX_valid, weights):\n",
    "    \"\"\"\n",
    "    Return the F1-score achieved with the predictions of a validation set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_valid : np.ndarray\n",
    "        Vector with the validation labels.\n",
    "    tX_valid : np.ndarray\n",
    "        Array with the validation samples as rows and the features as columns.\n",
    "    weights : np.ndarray\n",
    "        Vector containing the weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        F1-score for this configuration (the closer to 1 the better)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] Wikipedia entry for 'Precision and recall'\n",
    "        https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "    [2] Wikipedia entry for 'F-score'\n",
    "        https://en.wikipedia.org/wiki/F-score\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = predict_labels(weights, tX_valid) # Obtaining the predictions\n",
    "\n",
    "    # Calculating number of true positives, false positives, and false negatives\n",
    "    num_tp = np.sum((y_valid == 1) & (y_pred == 1))\n",
    "    num_fp = np.sum((y_valid == -1) & (y_pred == 1))\n",
    "    num_fn = np.sum((y_valid == 1) & (y_pred == -1))\n",
    "    precision = num_tp / (num_tp + num_fp)\n",
    "    recall = num_tp / (num_tp + num_fn)\n",
    "    f1 = np.round(2 * precision * recall / (precision + recall), 4)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_valid, tX_valid, weights):\n",
    "    \"\"\"\n",
    "    Return the accuracy achieved with the predictions of a validation set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_valid : np.ndarray\n",
    "        Vector with the validation labels.\n",
    "    tX_valid : np.ndarray\n",
    "        Array with the validation samples as rows and the features as columns.\n",
    "    weights : np.ndarray\n",
    "        Vector containing the weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "        Accuracy for this configuration (the closer to 1 the better)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [3] Wikipedia entry for 'Accuracy and precision'\n",
    "        https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = predict_labels(weights, tX_valid) # Obtaining the predictions\n",
    "    num_tp_tn = np.sum(y_valid == y_pred) # Calculating number of true positives and negatives\n",
    "    acc = np.round(num_tp_tn / len(y_valid), 4)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_predictions(y_valid, y_pred):\n",
    "    \"\"\"\n",
    "    Return the F1-score achieved with the predictions of a validation set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_valid : np.ndarray\n",
    "        Vector with the validation labels.\n",
    "    tX_valid : np.ndarray\n",
    "        Array with the validation samples as rows and the features as columns.\n",
    "    y_pred : np.ndarray\n",
    "        Vector containing the predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        F1-score for this configuration (the closer to 1 the better)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] Wikipedia entry for the Precision and recall\n",
    "        https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "    [2] Wikipedia entry for F-score\n",
    "        https://en.wikipedia.org/wiki/F-score\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating number of true positives, false positives, and false negatives\n",
    "    num_tp = np.sum((y_valid == 1) & (y_pred == 1))\n",
    "    num_fp = np.sum((y_valid == -1) & (y_pred == 1))\n",
    "    num_fn = np.sum((y_valid == 1) & (y_pred == -1))\n",
    "\n",
    "    precision = num_tp / (num_tp + num_fp)\n",
    "    recall = num_tp / (num_tp + num_fn)\n",
    "    f1 = np.round(2 * precision * recall / (precision + recall),4)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy preprocessing testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score achieved with 'ridge_regression', dataset  normal  : F1 =  0.6655\n",
      "Accuracy achieved with 'ridge_regression', dataset  normal  : accuracy =  0.718\n",
      "F1-score achieved with 'ridge_regression', dataset  zeros  : F1 =  0.6658\n",
      "Accuracy achieved with 'ridge_regression', dataset  zeros  : accuracy =  0.7192\n",
      "F1-score achieved with 'ridge_regression', dataset  mean  : F1 =  0.6652\n",
      "Accuracy achieved with 'ridge_regression', dataset  mean  : accuracy =  0.7194\n",
      "F1-score achieved with 'ridge_regression', dataset  median  : F1 =  0.6652\n",
      "Accuracy achieved with 'ridge_regression', dataset  median  : accuracy =  0.7197\n",
      "F1-score achieved with 'ridge_regression', dataset  binary  : F1 =  0.6655\n",
      "Accuracy achieved with 'ridge_regression', dataset  binary  : accuracy =  0.7182\n",
      "F1-score achieved with 'ridge_regression', dataset  deleted  : F1 =  0.6595\n",
      "Accuracy achieved with 'ridge_regression', dataset  deleted  : accuracy =  0.7069\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "for key in train_dict:\n",
    "    weights, loss = ridge_regression(y_train, train_dict[key], lambda_)\n",
    "    f1 = f1_score(y_valid, valid_dict[key], weights)\n",
    "    acc = accuracy(y_valid, valid_dict[key], weights)\n",
    "    print(\"F1-score achieved with 'ridge_regression', dataset \", key, \" : F1 = \", f1)\n",
    "    print(\"Accuracy achieved with 'ridge_regression', dataset \", key, \" : accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_score(regressor, y, tX, param, k_fold, score='f1'):    \n",
    "    \"\"\"\n",
    "    'k_fold' cross validate a regressor with parameters 'param', data '(y, TX)' \n",
    "    and score the predictions with 'score'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : String\n",
    "        String that defines which regressor to use\n",
    "    y : np.ndarray\n",
    "        Vector with the labels.\n",
    "    tX : np.ndarray\n",
    "        Array with the samples as rows and the features as columns.\n",
    "    param : np.ndarray\n",
    "        Parameters on weight, gamma and max_iterations\n",
    "    k_fold : Integer\n",
    "        Integer that defines how many folds to make\n",
    "    score : float\n",
    "        Float of a given performance score (e.g. f1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean : float\n",
    "    std : float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fold_size = int(len(y) / k_fold)\n",
    "    scores = []\n",
    "    rand_ind = np.random.permutation(np.arange(len(y)))\n",
    "    for k in range(k_fold):\n",
    "        valid_ind = rand_ind[k*fold_size:(k+1)*fold_size] # Splitting data set into validation and training set (for current fold)\n",
    "        tX_train, y_train = tX[valid_ind], y[valid_ind]\n",
    "        tX_valid, y_valid = np.delete(tX, valid_ind, axis=0), np.delete(y, valid_ind, axis=0)\n",
    "        weights, _ = eval(regressor)(y_train, tX_train, **param) # Fitting/scoring regressor with the tarining/validation set\n",
    "        if score == 'f1': # Scoring according to the scoring criterion\n",
    "            scores.append(f1_score(y_valid, tX_valid, weights))\n",
    "        else:\n",
    "            scores.append(accuracy(y_valid, tX_valid, weights))\n",
    "    return np.mean(scores), np.std(scores, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_degrees(regressor, compute_loss, y_train, tX_train, y_valid, tX_valid, param, k_fold, degrees):    \n",
    "    \"\"\"\n",
    "    'k_fold' cross validate a regressor with parameters 'param', data '(y, TX)' \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : String\n",
    "        String that defines which regressor to use\n",
    "    compute_loss : String\n",
    "        String that defines which loss to use\n",
    "    y : np.ndarray\n",
    "        Vector with the labels.\n",
    "    tX : np.ndarray\n",
    "        Array with the samples as rows and the features as columns.\n",
    "    param : np.ndarray\n",
    "        Parameters on weight, gamma and max_iterations\n",
    "    k_fold : Integer\n",
    "        Integer that defines how many folds to make\n",
    "    degrees : list or int\n",
    "        List (or int) with the polynomial degrees (or maximum polynomial degree)\n",
    "        that should be used as basis elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    training_mean : float\n",
    "    valid_mean : float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fold_size = int(len(y) / k_fold)\n",
    "    rand_ind = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "    losses_tr = []\n",
    "    losses_va = []\n",
    "\n",
    "    for k in range(k_fold):\n",
    "         # Splitting data set into validation and training set (for current fold)\n",
    "        valid_ind = rand_ind[k*fold_size:(k+1)*fold_size]\n",
    "        tX_train, y_train = tX[valid_ind], y[valid_ind]\n",
    "        tX_valid, y_valid = np.delete(tX, valid_ind, axis=0), np.delete(y, valid_ind, axis=0)\n",
    "\n",
    "        # Form the data with polynomial degree\n",
    "        tX_train = polynomial_basis(tX_train, degrees, std=True)\n",
    "        tX_valid = polynomial_basis(tX_valid, degrees, std=True)\n",
    "\n",
    "        # Train the model and calculate the loss for train and test data\n",
    "        if (regressor != 'least_squares' and regressor != 'ridge_regression'):   #for least squares and ridge regression\n",
    "            param['initial_w'] = np.ones(tX_train.shape[1])\n",
    "        weights, train_loss = eval(regressor)(y_train, tX_train, **param)\n",
    "        losses_tr.append(train_loss)\n",
    "        losses_va.append( eval(compute_loss)(y_valid, tX_valid, weights))\n",
    "                 \n",
    "    return np.mean(losses_va), np.std(losses_va), np.mean(losses_tr), np.std(losses_tr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_degree_params(regressor, compute_loss, y_train, tX_train, y_valid, tX_valid, max_iters, degrees, k_fold, params, verbose = True):\n",
    "    '''\n",
    "    TODO: Description \n",
    "    '''\n",
    "    # For each degree we compute the best lambda and the associated error\n",
    "    best_degree = float('inf')\n",
    "    if (compute_loss == 'accuracy' or compute_loss == 'f1_score'):\n",
    "        best_loss = float('-inf')\n",
    "    else:\n",
    "        best_loss = float('inf')\n",
    "    best_param = {}\n",
    "    \n",
    "    for degree in degrees:\n",
    "        if verbose:\n",
    "            print(\"Degree: \", degree)\n",
    "        for param in params:\n",
    "            loss_te, _, _, _ = cross_validate_degrees(regressor, compute_loss, y_train, tX_train, y_valid, tX_valid, param, k_fold, degree)\n",
    "            if verbose:\n",
    "                print(\"loss: \", loss_te)\n",
    "            if (compute_loss == 'accuracy' or compute_loss == 'f1_score'):\n",
    "                if loss_te > best_loss:\n",
    "                    best_loss = loss_te\n",
    "                    best_degree = degree\n",
    "                    best_param = param  \n",
    "            else: \n",
    "                if loss_te < best_loss:\n",
    "                    best_loss = loss_te\n",
    "                    best_degree = degree\n",
    "                    best_param = param\n",
    "\n",
    "        # Least squares has no parameters, we just need to find the optimal degree\n",
    "        if (params == []):\n",
    "            loss_te, _, _, _ = cross_validate_degrees(regressor, compute_loss, y_train, tX_train, y_valid, tX_valid, {}, k_fold, degree)\n",
    "            if verbose:\n",
    "                print(\"loss: \", loss_te)\n",
    "            if (compute_loss == 'accuracy' or compute_loss == 'f1_score'):\n",
    "                if loss_te > best_loss:\n",
    "                    best_loss = loss_te\n",
    "                    best_degree = degree \n",
    "            else: \n",
    "                if loss_te < best_loss:\n",
    "                    best_loss = loss_te\n",
    "                    best_degree = degree\n",
    "\n",
    "    \n",
    "    print(\"Best parameters set for '\", regressor, \"' using '\", compute_loss, \"' was:\")\n",
    "    print(\"degree = \", best_degree)\n",
    "    print(\"param = \", best_param)\n",
    "    print(\"Mean \", compute_loss, \" score: \", best_loss)\n",
    "\n",
    "    return best_degree, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_parameters(regressor, y, tX, params, k_fold, score):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find best parameters from a list of parameters with k_fold cross validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : String\n",
    "        String that defines which regressor to use\n",
    "    y : np.ndarray\n",
    "        Vector with the labels.\n",
    "    tX : np.ndarray\n",
    "        Array with the samples as rows and the features as columns.\n",
    "    param : np.ndarray\n",
    "        Parameters on weight, gamma and max_iterations\n",
    "    k_fold : Integer\n",
    "        Integer that defines how many folds to make\n",
    "    score : float\n",
    "        Float of a given performance score (e.g. f1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    max_std_score = 0.0\n",
    "    max_mean_score = 0.0\n",
    "    max_param = {}\n",
    "    for param in params:\n",
    "        mean_score, std_score = cross_validate_score(regressor, y, tX, param, k_fold, score)\n",
    "        if mean_score > max_mean_score: # Checking for the best score\n",
    "            max_mean_score = mean_score\n",
    "            max_std_score = std_score\n",
    "            max_param = param\n",
    "    print(\"Best parameter set for '\", regressor, \"' was:\")\n",
    "    print(\"param = \", max_param)\n",
    "    print(\"Mean \", score, \" score: \", max_mean_score, \" +/- \", max_std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n",
      "loss:  0.5125249999999999\n",
      "loss:  0.49794999999999995\n",
      "loss:  0.534225\n",
      "loss:  0.653775\n",
      "loss:  0.725175\n",
      "loss:  0.741025\n",
      "loss:  0.743575\n",
      "loss:  0.62855\n",
      "loss:  0.628525\n",
      "loss:  0.628525\n",
      "Degree:  1\n",
      "loss:  0.5126\n",
      "loss:  0.497825\n",
      "loss:  0.5342\n",
      "loss:  0.6536000000000001\n",
      "loss:  0.725125\n",
      "loss:  0.7409\n",
      "loss:  0.7434\n",
      "loss:  0.628525\n",
      "loss:  0.628525\n",
      "loss:  0.628525\n",
      "Degree:  2\n",
      "loss:  0.48260000000000003\n",
      "loss:  0.4979\n",
      "loss:  0.54395\n",
      "loss:  0.647075\n",
      "loss:  0.721425\n",
      "loss:  0.7521249999999999\n",
      "loss:  0.628275\n",
      "loss:  0.6283\n",
      "loss:  0.6282749999999999\n",
      "loss:  0.6282249999999999\n",
      "Degree:  3\n",
      "loss:  0.49985\n",
      "loss:  0.50965\n",
      "loss:  0.5453\n",
      "loss:  0.664125\n",
      "loss:  0.74275\n",
      "loss:  0.7592\n",
      "loss:  0.628275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivanb\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6283\n",
      "loss:  0.628275\n",
      "loss:  0.6283\n",
      "Degree:  4\n",
      "loss:  0.49655000000000005\n",
      "loss:  0.5129\n",
      "loss:  0.5612249999999999\n",
      "loss:  0.6913\n",
      "loss:  0.75095\n",
      "loss:  0.76205\n",
      "loss:  0.628025\n",
      "loss:  0.6280249999999999\n",
      "loss:  0.62805\n",
      "loss:  0.0\n",
      "Degree:  5\n",
      "loss:  0.504875\n",
      "loss:  0.5158750000000001\n",
      "loss:  0.557275\n",
      "loss:  0.7013750000000001\n",
      "loss:  0.7495499999999999\n",
      "loss:  0.628025\n",
      "loss:  0.62805\n",
      "loss:  0.628\n",
      "loss:  0.628025\n",
      "loss:  0.0\n",
      "Degree:  6\n",
      "loss:  0.5055000000000001\n",
      "loss:  0.5233749999999999\n",
      "loss:  0.5748\n",
      "loss:  0.7057\n",
      "loss:  0.748275\n",
      "loss:  0.627775\n",
      "loss:  0.6278\n",
      "loss:  0.627775\n",
      "loss:  0.6277750000000001\n",
      "loss:  0.0\n",
      "Degree:  7\n",
      "loss:  0.510775\n",
      "loss:  0.524675\n",
      "loss:  0.5637749999999999\n",
      "loss:  0.7088249999999999\n",
      "loss:  0.749325\n",
      "loss:  0.6276999999999999\n",
      "loss:  0.62765\n",
      "loss:  0.6277\n",
      "loss:  0.627675\n",
      "loss:  0.0\n",
      "Best parameters set for ' least_squares_GD ' using ' accuracy ' was:\n",
      "degree =  4\n",
      "param =  {'max_iters': 200, 'gamma': 0.046415888336127774, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.76205\n",
      "least_squares_GD, optimal degree:  4\n",
      "least_squares_GD, optimal gamma:  0.046415888336127774\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'least_squares_GD'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 200\n",
    "k_fold = 4\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma} for gamma in np.logspace(-3, 0, 10)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "GD_deg, GD_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('least_squares_GD, optimal degree: ', GD_deg)\n",
    "print('least_squares_GD, optimal gamma: ', GD_param['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n",
      "loss:  0.59985\n",
      "loss:  0.5997250000000001\n",
      "loss:  0.599525\n",
      "loss:  0.5990000000000001\n",
      "loss:  0.5977250000000001\n",
      "loss:  0.5952\n",
      "loss:  0.58795\n",
      "loss:  0.5678000000000001\n",
      "loss:  0.5283249999999999\n",
      "loss:  0.498575\n",
      "Degree:  1\n",
      "loss:  0.599825\n",
      "loss:  0.5996999999999999\n",
      "loss:  0.5995250000000001\n",
      "loss:  0.599\n",
      "loss:  0.5977250000000001\n",
      "loss:  0.5952\n",
      "loss:  0.5879\n",
      "loss:  0.56795\n",
      "loss:  0.52825\n",
      "loss:  0.499225\n",
      "Degree:  2\n",
      "loss:  0.561375\n",
      "loss:  0.560875\n",
      "loss:  0.560025\n",
      "loss:  0.5578749999999999\n",
      "loss:  0.553525\n",
      "loss:  0.542925\n",
      "loss:  0.523775\n",
      "loss:  0.497625\n",
      "loss:  0.48185\n",
      "loss:  0.49145\n",
      "Degree:  3\n",
      "loss:  0.587075\n",
      "loss:  0.5866250000000001\n",
      "loss:  0.58595\n",
      "loss:  0.5842499999999999\n",
      "loss:  0.5812499999999999\n",
      "loss:  0.572725\n",
      "loss:  0.55035\n",
      "loss:  0.512525\n",
      "loss:  0.5013\n",
      "loss:  0.5138\n",
      "Degree:  4\n",
      "loss:  0.558925\n",
      "loss:  0.5582750000000001\n",
      "loss:  0.5560499999999999\n",
      "loss:  0.55245\n",
      "loss:  0.545375\n",
      "loss:  0.5309999999999999\n",
      "loss:  0.508375\n",
      "loss:  0.48745000000000005\n",
      "loss:  0.493025\n",
      "loss:  0.5152749999999999\n",
      "Degree:  5\n",
      "loss:  0.5816\n",
      "loss:  0.581\n",
      "loss:  0.579925\n",
      "loss:  0.577475\n",
      "loss:  0.5701999999999999\n",
      "loss:  0.5525\n",
      "loss:  0.5248\n",
      "loss:  0.49995\n",
      "loss:  0.509675\n",
      "loss:  0.5193\n",
      "Degree:  6\n",
      "loss:  0.5531999999999999\n",
      "loss:  0.5519000000000001\n",
      "loss:  0.550175\n",
      "loss:  0.546125\n",
      "loss:  0.53755\n",
      "loss:  0.5220750000000001\n",
      "loss:  0.504125\n",
      "loss:  0.501275\n",
      "loss:  0.50755\n",
      "loss:  0.490675\n",
      "Degree:  7\n",
      "loss:  0.577375\n",
      "loss:  0.57665\n",
      "loss:  0.575025\n",
      "loss:  0.5698500000000001\n",
      "loss:  0.558025\n",
      "loss:  0.5381750000000001\n",
      "loss:  0.51145\n",
      "loss:  0.50235\n",
      "loss:  0.513325\n",
      "loss:  0.506625\n",
      "Best parameters set for ' least_squares_SGD ' using ' accuracy ' was:\n",
      "degree =  0\n",
      "param =  {'max_iters': 20000, 'gamma': 0.001, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.59985\n",
      "least_squares_SGD, optimal degree:  0\n",
      "least_squares_SGD, optimal gamma:  0.001\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'least_squares_SGD'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 20000\n",
    "k_fold = 4\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma} for gamma in np.logspace(-3, 0, 10)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "SGD_deg, SGD_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('least_squares_SGD, optimal degree: ', SGD_deg)\n",
    "print('least_squares_SGD, optimal gamma: ', SGD_param['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 8)\n",
      "Degree:  0\n",
      "loss:  0.627775\n",
      "Degree:  1\n",
      "loss:  0.66855\n",
      "Degree:  2\n",
      "loss:  0.47709999999999997\n",
      "Degree:  3\n",
      "loss:  0.499675\n",
      "Degree:  4\n",
      "loss:  0.47364999999999996\n",
      "Degree:  5\n",
      "loss:  0.500625\n",
      "Degree:  6\n",
      "loss:  0.492625\n",
      "Degree:  7\n",
      "loss:  0.47352500000000003\n",
      "Best parameters set for ' least_squares ' using ' accuracy ' was:\n",
      "degree =  1\n",
      "param =  {}\n",
      "Mean  accuracy  score:  0.66855\n",
      "ls_squares_SGD, optimal degree:  1\n",
      "----------------------------------------------------\n",
      "------------------ NO OFFSET TERM ------------------\n",
      "----------------------------------------------------\n",
      "Degree:  range(1, 2)\n",
      "loss:  0.46735\n",
      "Degree:  range(1, 3)\n",
      "loss:  0.5188750000000001\n",
      "Degree:  range(1, 4)\n",
      "loss:  0.4998\n",
      "Degree:  range(1, 5)\n",
      "loss:  0.482475\n",
      "Degree:  range(1, 6)\n",
      "loss:  0.5484\n",
      "Degree:  range(1, 7)\n",
      "loss:  0.5500999999999999\n",
      "Degree:  range(1, 8)\n",
      "loss:  0.500375\n",
      "Best parameters set for ' least_squares ' using ' accuracy ' was:\n",
      "degree =  range(1, 7)\n",
      "param =  {}\n",
      "Mean  accuracy  score:  0.5500999999999999\n",
      "ls_squares_SGD, optimal degree with no offset:  range(1, 7)\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'least_squares'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = None\n",
    "k_fold = 4\n",
    "params = []\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "ls_deg, ls_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('ls_squares_SGD, optimal degree: ', ls_deg)\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "print('------------------ NO OFFSET TERM ------------------')\n",
    "print('----------------------------------------------------')\n",
    "degrees = []\n",
    "for deg in range(1,8):\n",
    "    degrees.append(range(1, deg+1))\n",
    "\n",
    "ls_deg, ls_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('ls_squares_SGD, optimal degree with no offset: ', ls_deg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n",
      "loss:  0.743975\n",
      "loss:  0.7438\n",
      "loss:  0.743675\n",
      "loss:  0.743325\n",
      "loss:  0.742475\n",
      "loss:  0.7399249999999999\n",
      "loss:  0.735425\n",
      "loss:  0.7272\n",
      "loss:  0.71615\n",
      "loss:  0.701375\n",
      "Degree:  1\n",
      "loss:  0.7441\n",
      "loss:  0.743975\n",
      "loss:  0.743625\n",
      "loss:  0.743375\n",
      "loss:  0.7425\n",
      "loss:  0.7401249999999999\n",
      "loss:  0.735225\n",
      "loss:  0.7271\n",
      "loss:  0.7160500000000001\n",
      "loss:  0.701425\n",
      "Degree:  2\n",
      "loss:  0.760825\n",
      "loss:  0.7611000000000001\n",
      "loss:  0.7602\n",
      "loss:  0.7584\n",
      "loss:  0.7563500000000001\n",
      "loss:  0.753425\n",
      "loss:  0.750175\n",
      "loss:  0.7453249999999999\n",
      "loss:  0.737425\n",
      "loss:  0.7214\n",
      "Degree:  3\n",
      "loss:  0.768125\n",
      "loss:  0.76595\n",
      "loss:  0.765225\n",
      "loss:  0.763\n",
      "loss:  0.75945\n",
      "loss:  0.7552500000000001\n",
      "loss:  0.7502\n",
      "loss:  0.74425\n",
      "loss:  0.73515\n",
      "loss:  0.7188999999999999\n",
      "Degree:  4\n",
      "loss:  0.767325\n",
      "loss:  0.76665\n",
      "loss:  0.764975\n",
      "loss:  0.763025\n",
      "loss:  0.7602249999999999\n",
      "loss:  0.756325\n",
      "loss:  0.75135\n",
      "loss:  0.7447999999999999\n",
      "loss:  0.7369\n",
      "loss:  0.7219749999999999\n",
      "Degree:  5\n",
      "loss:  0.7685\n",
      "loss:  0.76825\n",
      "loss:  0.76625\n",
      "loss:  0.763425\n",
      "loss:  0.76075\n",
      "loss:  0.756725\n",
      "loss:  0.751825\n",
      "loss:  0.7448750000000001\n",
      "loss:  0.735525\n",
      "loss:  0.7203\n",
      "Degree:  6\n",
      "loss:  0.76895\n",
      "loss:  0.7685249999999999\n",
      "loss:  0.766575\n",
      "loss:  0.7640250000000001\n",
      "loss:  0.7607\n",
      "loss:  0.7567249999999999\n",
      "loss:  0.751625\n",
      "loss:  0.7447250000000001\n",
      "loss:  0.7361249999999999\n",
      "loss:  0.721325\n",
      "Degree:  7\n",
      "loss:  0.768575\n",
      "loss:  0.768175\n",
      "loss:  0.7666\n",
      "loss:  0.7645500000000001\n",
      "loss:  0.7611249999999999\n",
      "loss:  0.7569\n",
      "loss:  0.75145\n",
      "loss:  0.7445249999999999\n",
      "loss:  0.735225\n",
      "loss:  0.7201500000000001\n",
      "Best parameters set for ' ridge_regression ' using ' accuracy ' was:\n",
      "degree =  6\n",
      "param =  {'lambda_': 0.001}\n",
      "Mean  accuracy  score:  0.76895\n",
      "least_squares_GD, optimal degree:  6\n",
      "least_squares_GD, optimal gamma:  0.001\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'ridge_regression'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = None\n",
    "k_fold = 4\n",
    "params = [{'lambda_': lambda_} for lambda_ in np.logspace(-3, 0, 10)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "ridge_deg, ridge_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('least_squares_GD, optimal degree: ', ridge_deg)\n",
    "print('least_squares_GD, optimal gamma: ', ridge_param['lambda_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n",
      "loss:  0.512575\n",
      "loss:  0.5125\n",
      "loss:  0.511575\n",
      "loss:  0.499825\n",
      "loss:  0.6541250000000001\n",
      "loss:  0.658125\n",
      "loss:  0.6941\n",
      "loss:  0.58555\n",
      "loss:  0.7434250000000001\n",
      "loss:  0.740175\n",
      "loss:  0.698875\n",
      "loss:  0.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ivanb\\Documents\\GitHub\\ML_project1\\implementations.py:645: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.mean(e**2) / 2\n",
      "C:\\Users\\ivanb\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:229: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.628525\n",
      "loss:  0.6285499999999999\n",
      "loss:  0.628525\n",
      "loss:  0.628525\n",
      "Degree:  1\n",
      "loss:  0.5125500000000001\n",
      "loss:  0.5125500000000001\n",
      "loss:  0.5115\n",
      "loss:  0.49975\n",
      "loss:  0.6541250000000001\n",
      "loss:  0.6581250000000001\n",
      "loss:  0.6940500000000001\n",
      "loss:  0.5669500000000001\n",
      "loss:  0.743425\n",
      "loss:  0.7402\n",
      "loss:  0.7005\n",
      "loss:  0.631175\n",
      "loss:  0.628525\n",
      "loss:  0.6285\n",
      "loss:  0.6285\n",
      "loss:  0.6285\n",
      "Degree:  2\n",
      "loss:  0.482575\n",
      "loss:  0.4823\n",
      "loss:  0.482225\n",
      "loss:  0.479925\n",
      "loss:  0.647825\n",
      "loss:  0.6514249999999999\n",
      "loss:  0.6760999999999999\n",
      "loss:  0.60785\n",
      "loss:  0.6283000000000001\n",
      "loss:  0.6283\n",
      "loss:  0.62825\n",
      "loss:  0.628325\n",
      "loss:  0.628275\n",
      "loss:  0.628275\n",
      "loss:  0.62825\n",
      "loss:  0.6282749999999999\n",
      "Degree:  3\n",
      "loss:  0.49997499999999995\n",
      "loss:  0.49997500000000006\n",
      "loss:  0.49975\n",
      "loss:  0.49572499999999997\n",
      "loss:  0.664875\n",
      "loss:  0.66995\n",
      "loss:  0.692875\n",
      "loss:  0.554325\n",
      "loss:  0.628275\n",
      "loss:  0.6282749999999999\n",
      "loss:  0.6283000000000001\n",
      "loss:  0.6283\n",
      "loss:  0.628275\n",
      "loss:  0.6283\n",
      "loss:  0.6283000000000001\n",
      "loss:  0.62825\n",
      "Degree:  4\n",
      "loss:  0.49670000000000003\n",
      "loss:  0.49692500000000006\n",
      "loss:  0.49670000000000003\n",
      "loss:  0.4994\n",
      "loss:  0.69185\n",
      "loss:  0.6960000000000001\n",
      "loss:  0.70495\n",
      "loss:  0.5727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivanb\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:232: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.62805\n",
      "loss:  0.6280749999999999\n",
      "loss:  0.62805\n",
      "loss:  0.6280249999999999\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "Degree:  5\n",
      "loss:  0.50465\n",
      "loss:  0.504825\n",
      "loss:  0.5045999999999999\n",
      "loss:  0.504625\n",
      "loss:  0.7023250000000001\n",
      "loss:  0.707275\n",
      "loss:  0.726375\n",
      "loss:  0.5299999999999999\n",
      "loss:  0.62805\n",
      "loss:  0.62805\n",
      "loss:  0.6280499999999999\n",
      "loss:  0.628025\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "Degree:  6\n",
      "loss:  0.505575\n",
      "loss:  0.5058750000000001\n",
      "loss:  0.50615\n",
      "loss:  0.5101249999999999\n",
      "loss:  0.708775\n",
      "loss:  0.7156750000000001\n",
      "loss:  0.7314\n",
      "loss:  0.5704\n",
      "loss:  0.627775\n",
      "loss:  0.627775\n",
      "loss:  0.627825\n",
      "loss:  0.627775\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "Degree:  7\n",
      "loss:  0.5101249999999999\n",
      "loss:  0.5105500000000001\n",
      "loss:  0.509825\n",
      "loss:  0.511625\n",
      "loss:  0.709125\n",
      "loss:  0.7135\n",
      "loss:  0.734175\n",
      "loss:  0.469975\n",
      "loss:  0.627675\n",
      "loss:  0.627675\n",
      "loss:  0.627675\n",
      "loss:  0.6276999999999999\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "loss:  0.0\n",
      "Best parameters set for ' lasso_SD ' using ' accuracy ' was:\n",
      "degree =  0\n",
      "param =  {'max_iters': 200, 'gamma': 0.1, 'lambda_': 0.01, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.7434250000000001\n",
      "least_squares_SGD, optimal degree:  0\n",
      "least_squares_SGD, optimal gamma:  0.1\n",
      "least_squares_SGD, optimal lambda_:  0.01\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "#Setting tuning\n",
    "regressor = 'lasso_SD'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 200\n",
    "k_fold = 4\n",
    "gammas = np.logspace(-3, 0, 4)\n",
    "lambdas = np.logspace(-2, 1, 4)\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma, 'lambda_': lambda_} for gamma, lambda_ in product(gammas, lambdas)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "lasso_SD_deg, lasso_SD_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('least_squares_SGD, optimal degree: ', lasso_SD_deg)\n",
    "print('least_squares_SGD, optimal gamma: ', lasso_SD_param['gamma'])\n",
    "print('least_squares_SGD, optimal lambda_: ', lasso_SD_param['lambda_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n",
      "loss:  0.6438\n",
      "loss:  0.65635\n",
      "loss:  0.66035\n",
      "loss:  0.6613\n",
      "loss:  0.6613500000000001\n",
      "loss:  0.661\n",
      "loss:  0.661225\n",
      "loss:  0.661225\n",
      "loss:  0.661275\n",
      "loss:  0.661025\n",
      "Degree:  1\n",
      "loss:  0.62915\n",
      "loss:  0.656275\n",
      "loss:  0.6602\n",
      "loss:  0.6605000000000001\n",
      "loss:  0.66125\n",
      "loss:  0.661175\n",
      "loss:  0.66135\n",
      "loss:  0.661475\n",
      "loss:  0.66125\n",
      "loss:  0.661625\n",
      "Degree:  2\n",
      "loss:  0.658375\n",
      "loss:  0.660525\n",
      "loss:  0.6617500000000001\n",
      "loss:  0.6618\n",
      "loss:  0.66225\n",
      "loss:  0.66235\n",
      "loss:  0.6618999999999999\n",
      "loss:  0.662325\n",
      "loss:  0.6620250000000001\n",
      "loss:  0.661675\n",
      "Degree:  3\n",
      "loss:  0.6584000000000001\n",
      "loss:  0.6504\n",
      "loss:  0.6513249999999999\n",
      "loss:  0.6515\n",
      "loss:  0.651475\n",
      "loss:  0.651325\n",
      "loss:  0.651325\n",
      "loss:  0.651\n",
      "loss:  0.6512249999999999\n",
      "loss:  0.6516\n",
      "Degree:  4\n",
      "loss:  0.612825\n",
      "loss:  0.648425\n",
      "loss:  0.648825\n",
      "loss:  0.6484500000000001\n",
      "loss:  0.648475\n",
      "loss:  0.6485000000000001\n",
      "loss:  0.6487499999999999\n",
      "loss:  0.64845\n",
      "loss:  0.6488\n",
      "loss:  0.648725\n",
      "Degree:  5\n",
      "loss:  0.6444749999999999\n",
      "loss:  0.645675\n",
      "loss:  0.6471750000000001\n",
      "loss:  0.646525\n",
      "loss:  0.6467\n",
      "loss:  0.6461250000000001\n",
      "loss:  0.6471\n",
      "loss:  0.64635\n",
      "loss:  0.646625\n",
      "loss:  0.64705\n",
      "Degree:  6\n",
      "loss:  0.5999249999999999\n",
      "loss:  0.649825\n",
      "loss:  0.6543\n",
      "loss:  0.644625\n",
      "loss:  0.6446999999999999\n",
      "loss:  0.6448\n",
      "loss:  0.64475\n",
      "loss:  0.6447\n",
      "loss:  0.644725\n",
      "loss:  0.64445\n",
      "Degree:  7\n",
      "loss:  0.6432\n",
      "loss:  0.6419\n",
      "loss:  0.6449499999999999\n",
      "loss:  0.6493500000000001\n",
      "loss:  0.6491499999999999\n",
      "loss:  0.65805\n",
      "loss:  0.649275\n",
      "loss:  0.655975\n",
      "loss:  0.648925\n",
      "loss:  0.6472\n",
      "Best parameters set for ' logistic_regression ' using ' accuracy ' was:\n",
      "degree =  2\n",
      "param =  {'max_iters': 20, 'gamma': 0.05994842503189409, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.66235\n",
      "logistic_regression, optimal degree:  2\n",
      "logistic_regression, optimal gamma:  0.05994842503189409\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'logistic_regression'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 20\n",
    "k_fold = 4\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma} for gamma in np.logspace(-4, 1, 10)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "logistic_regression_deg, logistic_regression_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('logistic_regression, optimal degree: ', logistic_regression_deg)\n",
    "print('logistic_regression, optimal gamma: ', logistic_regression_param['gamma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n",
      "loss:  0.65985\n",
      "loss:  0.65985\n",
      "loss:  0.6591750000000001\n",
      "loss:  0.652225\n",
      "loss:  0.661075\n",
      "loss:  0.6606000000000001\n",
      "loss:  0.6537250000000001\n",
      "loss:  0.6393\n",
      "loss:  0.66055\n",
      "loss:  0.653575\n",
      "loss:  0.6393\n",
      "loss:  0.63035\n",
      "loss:  0.65355\n",
      "loss:  0.63925\n",
      "loss:  0.63035\n",
      "loss:  0.621325\n",
      "Degree:  1\n",
      "loss:  0.659875\n",
      "loss:  0.6599999999999999\n",
      "loss:  0.6590499999999999\n",
      "loss:  0.6522249999999999\n",
      "loss:  0.661175\n",
      "loss:  0.660175\n",
      "loss:  0.653575\n",
      "loss:  0.639275\n",
      "loss:  0.6603000000000001\n",
      "loss:  0.65335\n",
      "loss:  0.6392500000000001\n",
      "loss:  0.630325\n",
      "loss:  0.6535\n",
      "loss:  0.63925\n",
      "loss:  0.63035\n",
      "loss:  0.6213249999999999\n",
      "Degree:  2\n",
      "loss:  0.661475\n",
      "loss:  0.66125\n",
      "loss:  0.661125\n",
      "loss:  0.656075\n",
      "loss:  0.659725\n",
      "loss:  0.6611750000000001\n",
      "loss:  0.6567000000000001\n",
      "loss:  0.6365500000000001\n",
      "loss:  0.6613\n",
      "loss:  0.657175\n",
      "loss:  0.6363750000000001\n",
      "loss:  0.6292\n",
      "loss:  0.6562\n",
      "loss:  0.636475\n",
      "loss:  0.6292\n",
      "loss:  0.61405\n",
      "Degree:  3\n",
      "loss:  0.65125\n",
      "loss:  0.6510750000000001\n",
      "loss:  0.65035\n",
      "loss:  0.6470750000000001\n",
      "loss:  0.65125\n",
      "loss:  0.6509\n",
      "loss:  0.64715\n",
      "loss:  0.637375\n",
      "loss:  0.6506000000000001\n",
      "loss:  0.6469750000000001\n",
      "loss:  0.6372249999999999\n",
      "loss:  0.628525\n",
      "loss:  0.646925\n",
      "loss:  0.637325\n",
      "loss:  0.6285499999999999\n",
      "loss:  0.625275\n",
      "Degree:  4\n",
      "loss:  0.64835\n",
      "loss:  0.65\n",
      "loss:  0.648575\n",
      "loss:  0.6456\n",
      "loss:  0.6487999999999999\n",
      "loss:  0.6482\n",
      "loss:  0.6461\n",
      "loss:  0.6332\n",
      "loss:  0.6482\n",
      "loss:  0.6456000000000001\n",
      "loss:  0.633325\n",
      "loss:  0.628025\n",
      "loss:  0.645675\n",
      "loss:  0.632975\n",
      "loss:  0.6280249999999999\n",
      "loss:  0.6213\n",
      "Degree:  5\n",
      "loss:  0.647025\n",
      "loss:  0.647125\n",
      "loss:  0.6468750000000001\n",
      "loss:  0.6453\n",
      "loss:  0.646675\n",
      "loss:  0.646875\n",
      "loss:  0.645\n",
      "loss:  0.641375\n",
      "loss:  0.6469\n",
      "loss:  0.6453\n",
      "loss:  0.635\n",
      "loss:  0.6279\n",
      "loss:  0.6450750000000001\n",
      "loss:  0.635275\n",
      "loss:  0.627875\n",
      "loss:  0.626175\n",
      "Degree:  6\n",
      "loss:  0.6510750000000001\n",
      "loss:  0.6161\n",
      "loss:  0.629775\n",
      "loss:  0.6439250000000001\n",
      "loss:  0.6296999999999999\n",
      "loss:  0.643875\n",
      "loss:  0.6433\n",
      "loss:  0.6316750000000001\n",
      "loss:  0.65045\n",
      "loss:  0.6433499999999999\n",
      "loss:  0.6315\n",
      "loss:  0.627725\n",
      "loss:  0.6460999999999999\n",
      "loss:  0.631725\n",
      "loss:  0.6276999999999999\n",
      "loss:  0.621975\n",
      "Degree:  7\n",
      "loss:  0.643825\n",
      "loss:  0.64415\n",
      "loss:  0.6438999999999999\n",
      "loss:  0.6428\n",
      "loss:  0.6595\n",
      "loss:  0.6531250000000001\n",
      "loss:  0.651575\n",
      "loss:  0.63285\n",
      "loss:  0.652375\n",
      "loss:  0.656825\n",
      "loss:  0.633425\n",
      "loss:  0.627475\n",
      "loss:  0.647875\n",
      "loss:  0.6332\n",
      "loss:  0.627475\n",
      "loss:  0.6261749999999999\n",
      "Best parameters set for ' reg_logistic_regression ' using ' accuracy ' was:\n",
      "degree =  2\n",
      "param =  {'max_iters': 20, 'gamma': 0.001, 'lambda_': 0.01, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.661475\n",
      "reg_logistic_regression, optimal degree:  2\n",
      "logistic_regression, optimal gamma:  0.001\n",
      "logistic_regression, optimal lambda_:  0.01\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'reg_logistic_regression'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 20\n",
    "k_fold = 4\n",
    "gammas = np.logspace(-3, 0, 4)\n",
    "lambdas = np.logspace(-2, 1, 4)\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma, 'lambda_': lambda_} for gamma, lambda_ in product(gammas, lambdas)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "reg_logistic_regression_deg, reg_logistic_regression_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('reg_logistic_regression, optimal degree: ', reg_logistic_regression_deg)\n",
    "print('logistic_regression, optimal gamma: ', reg_logistic_regression_param['gamma'])\n",
    "print('logistic_regression, optimal lambda_: ', reg_logistic_regression_param['lambda_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"\n",
    "    apply the sigmoid function on t.\n",
    "    TODO: Description\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def calculate_loss_logistic(y, tx, w):\n",
    "    \"\"\"\n",
    "    compute the loss: negative log likelihood.\n",
    "    \"\"\"\n",
    "    sigma = sigmoid(tx @ w)\n",
    "    loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n",
    "    return loss\n",
    "\n",
    "def calculate_gradient_logistic(y, tx, w):\n",
    "    \"\"\"\n",
    "    compute the gradient of loss.\n",
    "    TODO: Description\n",
    "    \"\"\"\n",
    "    grad = np.dot(tx.T, (sigmoid(np.dot(tx, w)) - y))\n",
    "    return grad\n",
    "\n",
    "def m_logistic_regression(y, tX, initial_w=None, max_iters=100, gamma=0.1):\n",
    "    \"\"\"\n",
    "    TODO: Description\n",
    "    \"\"\"\n",
    "    if len(tX.shape) == 1: # Checking if 'tX' is a 1D array\n",
    "        tX = tX.reshape((-1, 1)) # consequently converting to a 2D array\n",
    "\n",
    "    # Zero vector for 'initial_w' if no initial value was specified\n",
    "    if initial_w is None:\n",
    "        initial_w = np.zeros(tX.shape[1])\n",
    "    \n",
    "    # Converting 1D arrays to 2D arrays\n",
    "    w = initial_w.reshape((-1, 1))\n",
    "    y = y.reshape((-1, 1))\n",
    " \n",
    "    for _ in range(max_iters):\n",
    "        grad = calculate_gradient_logistic(y, tX, w)\n",
    "        w = w - gamma * grad # Updating weights with scaled negative gradient\n",
    "    \n",
    "    # Computing loss for the weights of the final iteration\n",
    "    loss = calculate_loss_logistic(y, tX, w)\n",
    "    w = w.reshape(-1) # Converting weights back to 1D arrays\n",
    "    return w, loss\n",
    "\n",
    "def m_reg_logistic_regression(y, tX, initial_w=None, max_iters=100, gamma=0.1, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    TODO: Description\n",
    "    \"\"\"\n",
    "    if len(tX.shape) == 1: # Checking if 'tX' is a 1D array\n",
    "        tX = tX.reshape((-1, 1)) # consequently converting to a 2D array\n",
    "\n",
    "    # Zero vector for 'initial_w' if no initial value was specified\n",
    "    if initial_w is None:\n",
    "        initial_w = np.zeros(tX.shape[1])\n",
    "    \n",
    "    # Converting 1D arrays to 2D arrays\n",
    "    w = initial_w.reshape((-1, 1))\n",
    "    y = y.reshape((-1, 1))\n",
    " \n",
    "    for _ in range(max_iters):\n",
    "        grad = calculate_gradient_logistic(y, tX, w) + lambda_ * w\n",
    "        w = w - gamma * grad # Updating weights with scaled negative gradient\n",
    "    \n",
    "    # Computing loss for the weights of the final iteration\n",
    "    loss = calculate_loss_logistic(y, tX, w) + lambda_ / 2 * np.linalg.norm(w)\n",
    "    w = w.reshape(-1) # Converting weights back to 1D arrays\n",
    "    return w, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-a0efaedea64c>:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "<ipython-input-54-a0efaedea64c>:13: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n",
      "<ipython-input-54-a0efaedea64c>:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.703025\n",
      "loss:  0.702425\n",
      "loss:  0.702425\n",
      "loss:  0.7022999999999999\n",
      "loss:  0.702075\n",
      "loss:  0.702275\n",
      "loss:  0.702125\n",
      "loss:  0.7022499999999999\n",
      "loss:  0.7021750000000001\n",
      "loss:  0.7022999999999999\n",
      "Degree:  1\n",
      "loss:  0.70305\n",
      "loss:  0.7023999999999999\n",
      "loss:  0.702375\n",
      "loss:  0.702175\n",
      "loss:  0.70225\n",
      "loss:  0.7023\n",
      "loss:  0.70225\n",
      "loss:  0.702175\n",
      "loss:  0.7022\n",
      "loss:  0.7021999999999999\n",
      "Degree:  2\n",
      "loss:  0.7219500000000001\n",
      "loss:  0.7218\n",
      "loss:  0.721925\n",
      "loss:  0.7219249999999999\n",
      "loss:  0.7219\n",
      "loss:  0.721975\n",
      "loss:  0.7221\n",
      "loss:  0.72175\n",
      "loss:  0.7218\n",
      "loss:  0.72185\n",
      "Degree:  3\n",
      "loss:  0.726475\n",
      "loss:  0.7261500000000001\n",
      "loss:  0.7262500000000001\n",
      "loss:  0.7261\n",
      "loss:  0.726275\n",
      "loss:  0.7263000000000001\n",
      "loss:  0.726075\n",
      "loss:  0.7262500000000001\n",
      "loss:  0.72615\n",
      "loss:  0.72605\n",
      "Degree:  4\n",
      "loss:  0.730075\n",
      "loss:  0.730025\n",
      "loss:  0.730225\n",
      "loss:  0.730075\n",
      "loss:  0.730225\n",
      "loss:  0.730175\n",
      "loss:  0.7301\n",
      "loss:  0.730175\n",
      "loss:  0.73\n",
      "loss:  0.7300249999999999\n",
      "Degree:  5\n",
      "loss:  0.7328749999999999\n",
      "loss:  0.73285\n",
      "loss:  0.73285\n",
      "loss:  0.7330749999999999\n",
      "loss:  0.7328749999999999\n",
      "loss:  0.7327750000000001\n",
      "loss:  0.7326499999999999\n",
      "loss:  0.732875\n",
      "loss:  0.73275\n",
      "loss:  0.732875\n",
      "Degree:  6\n",
      "loss:  0.7353000000000001\n",
      "loss:  0.7353750000000001\n",
      "loss:  0.7355999999999999\n",
      "loss:  0.735675\n",
      "loss:  0.73585\n",
      "loss:  0.7356499999999999\n",
      "loss:  0.735625\n",
      "loss:  0.7356\n",
      "loss:  0.7354499999999999\n",
      "loss:  0.7354749999999999\n",
      "Degree:  7\n",
      "loss:  0.736675\n",
      "loss:  0.735725\n",
      "loss:  0.736125\n",
      "loss:  0.738475\n",
      "loss:  0.737225\n",
      "loss:  0.7364999999999999\n",
      "loss:  0.736525\n",
      "loss:  0.736575\n",
      "loss:  0.7357\n",
      "loss:  0.737525\n",
      "Best parameters set for ' m_logistic_regression ' using ' accuracy ' was:\n",
      "degree =  7\n",
      "param =  {'max_iters': 25, 'gamma': 0.004641588833612782, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.738475\n",
      "m_logistic_regression, optimal degree:  7\n",
      "m_logistic_regression, optimal gamma:  0.004641588833612782\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'm_logistic_regression'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 25\n",
    "k_fold = 4\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma} for gamma in np.logspace(-4, 1, 10)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "m_logistic_regression_deg, m_logistic_regression_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('m_logistic_regression, optimal degree: ', m_logistic_regression_deg)\n",
    "print('m_logistic_regression, optimal gamma: ', m_logistic_regression_param['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-a0efaedea64c>:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "<ipython-input-54-a0efaedea64c>:13: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n",
      "<ipython-input-54-a0efaedea64c>:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.7030000000000001\n",
      "loss:  0.7023999999999999\n",
      "loss:  0.702375\n",
      "loss:  0.70225\n",
      "loss:  0.702225\n",
      "loss:  0.702375\n",
      "loss:  0.7022250000000001\n",
      "loss:  0.7021499999999999\n",
      "loss:  0.702325\n",
      "loss:  0.7022250000000001\n",
      "Degree:  1\n",
      "loss:  0.703075\n",
      "loss:  0.7024499999999999\n",
      "loss:  0.702275\n",
      "loss:  0.702325\n",
      "loss:  0.7021999999999999\n",
      "loss:  0.7022250000000001\n",
      "loss:  0.7021999999999999\n",
      "loss:  0.7022499999999999\n",
      "loss:  0.702325\n",
      "loss:  0.7022999999999999\n",
      "Degree:  2\n",
      "loss:  0.7221500000000001\n",
      "loss:  0.7219\n",
      "loss:  0.721925\n",
      "loss:  0.7219\n",
      "loss:  0.721775\n",
      "loss:  0.722\n",
      "loss:  0.721975\n",
      "loss:  0.721775\n",
      "loss:  0.7219500000000001\n",
      "loss:  0.72195\n",
      "Degree:  3\n",
      "loss:  0.726325\n",
      "loss:  0.72625\n",
      "loss:  0.726425\n",
      "loss:  0.7262000000000001\n",
      "loss:  0.726\n",
      "loss:  0.726175\n",
      "loss:  0.726375\n",
      "loss:  0.7261500000000001\n",
      "loss:  0.72625\n",
      "loss:  0.7263000000000001\n",
      "Degree:  4\n",
      "loss:  0.7299\n",
      "loss:  0.7299500000000001\n",
      "loss:  0.7303\n",
      "loss:  0.7302\n",
      "loss:  0.7303000000000001\n",
      "loss:  0.73035\n",
      "loss:  0.7302\n",
      "loss:  0.7302250000000001\n",
      "loss:  0.7300500000000001\n",
      "loss:  0.730175\n",
      "Degree:  5\n",
      "loss:  0.73295\n",
      "loss:  0.7331\n",
      "loss:  0.732875\n",
      "loss:  0.732775\n",
      "loss:  0.7328749999999999\n",
      "loss:  0.7327\n",
      "loss:  0.732875\n",
      "loss:  0.7327750000000001\n",
      "loss:  0.733025\n",
      "loss:  0.7330000000000001\n",
      "Degree:  6\n",
      "loss:  0.73505\n",
      "loss:  0.7357250000000001\n",
      "loss:  0.7355750000000001\n",
      "loss:  0.7358500000000001\n",
      "loss:  0.7357\n",
      "loss:  0.7355250000000001\n",
      "loss:  0.7356999999999999\n",
      "loss:  0.7359\n",
      "loss:  0.7355\n",
      "loss:  0.73565\n",
      "Degree:  7\n",
      "loss:  0.7366\n",
      "loss:  0.7374750000000001\n",
      "loss:  0.7377\n",
      "loss:  0.736975\n",
      "loss:  0.7373000000000001\n",
      "loss:  0.737475\n",
      "loss:  0.73475\n",
      "loss:  0.7363999999999999\n",
      "loss:  0.736325\n",
      "loss:  0.738975\n",
      "Best parameters set for ' m_logistic_regression ' using ' accuracy ' was:\n",
      "degree =  7\n",
      "param =  {'max_iters': 25, 'gamma': 10.0, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.738975\n",
      "m_logistic_regression (median dataset), optimal degree:  7\n",
      "m_logistic_regression (median dataset), optimal gamma:  10.0\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'm_logistic_regression'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 25\n",
    "k_fold = 4\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma} for gamma in np.logspace(-4, 1, 10)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train_median\n",
    "tX_va = tX_valid_median\n",
    "\n",
    "m_logistic_regression_deg_median, m_logistic_regression_param_median = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('m_logistic_regression (median dataset), optimal degree: ', m_logistic_regression_deg_median)\n",
    "print('m_logistic_regression (median dataset), optimal gamma: ', m_logistic_regression_param_median['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-a0efaedea64c>:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "<ipython-input-54-a0efaedea64c>:13: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n",
      "<ipython-input-54-a0efaedea64c>:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = - np.sum(y * np.log( sigma ) + (1 - y) * np.log( 1 - sigma ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.70215\n",
      "loss:  0.7023\n",
      "loss:  0.702275\n",
      "loss:  0.7023250000000001\n",
      "loss:  0.702125\n",
      "loss:  0.702225\n",
      "loss:  0.7022250000000001\n",
      "loss:  0.70245\n",
      "loss:  0.7020249999999999\n",
      "loss:  0.70215\n",
      "loss:  0.7026249999999999\n",
      "loss:  0.630775\n",
      "loss:  0.7021499999999999\n",
      "loss:  0.7024\n",
      "loss:  0.6307499999999999\n",
      "loss:  0.5838000000000001\n",
      "Degree:  1\n",
      "loss:  0.7023\n",
      "loss:  0.702225\n",
      "loss:  0.70215\n",
      "loss:  0.7024250000000001\n",
      "loss:  0.702025\n",
      "loss:  0.7021999999999999\n",
      "loss:  0.702275\n",
      "loss:  0.70245\n",
      "loss:  0.7021999999999999\n",
      "loss:  0.7021999999999999\n",
      "loss:  0.7023999999999999\n",
      "loss:  0.6307750000000001\n",
      "loss:  0.702175\n",
      "loss:  0.7024999999999999\n",
      "loss:  0.63075\n",
      "loss:  0.58385\n",
      "Degree:  2\n",
      "loss:  0.721725\n",
      "loss:  0.721925\n",
      "loss:  0.72175\n",
      "loss:  0.7220249999999999\n",
      "loss:  0.7219\n",
      "loss:  0.72195\n",
      "loss:  0.721875\n",
      "loss:  0.721425\n",
      "loss:  0.7219\n",
      "loss:  0.72205\n",
      "loss:  0.7213499999999999\n",
      "loss:  0.629625\n",
      "loss:  0.721975\n",
      "loss:  0.721475\n",
      "loss:  0.629625\n",
      "loss:  0.575775\n",
      "Degree:  3\n",
      "loss:  0.726175\n",
      "loss:  0.72615\n",
      "loss:  0.7262000000000001\n",
      "loss:  0.7263749999999999\n",
      "loss:  0.72635\n",
      "loss:  0.7263\n",
      "loss:  0.7259749999999999\n",
      "loss:  0.718325\n",
      "loss:  0.7263\n",
      "loss:  0.726375\n",
      "loss:  0.7185250000000001\n",
      "loss:  0.628825\n",
      "loss:  0.726175\n",
      "loss:  0.71845\n",
      "loss:  0.62885\n",
      "loss:  0.61755\n",
      "Degree:  4\n",
      "loss:  0.7303000000000001\n",
      "loss:  0.730375\n",
      "loss:  0.730275\n",
      "loss:  0.7305\n",
      "loss:  0.7303\n",
      "loss:  0.7303\n",
      "loss:  0.73045\n",
      "loss:  0.7176999999999999\n",
      "loss:  0.7301\n",
      "loss:  0.73065\n",
      "loss:  0.7177\n",
      "loss:  0.62845\n",
      "loss:  0.7307250000000001\n",
      "loss:  0.7176750000000001\n",
      "loss:  0.628425\n",
      "loss:  0.5977250000000001\n",
      "Degree:  5\n",
      "loss:  0.7330500000000001\n",
      "loss:  0.7333500000000001\n",
      "loss:  0.7334750000000001\n",
      "loss:  0.734075\n",
      "loss:  0.7331\n",
      "loss:  0.7332\n",
      "loss:  0.7341000000000001\n",
      "loss:  0.709875\n",
      "loss:  0.7334499999999999\n",
      "loss:  0.7341\n",
      "loss:  0.710025\n",
      "loss:  0.6280749999999999\n",
      "loss:  0.734125\n",
      "loss:  0.709775\n",
      "loss:  0.6281\n",
      "loss:  0.622025\n",
      "Degree:  6\n",
      "loss:  0.73675\n",
      "loss:  0.7366750000000001\n",
      "loss:  0.736825\n",
      "loss:  0.7386250000000001\n",
      "loss:  0.736675\n",
      "loss:  0.73695\n",
      "loss:  0.73865\n",
      "loss:  0.70775\n",
      "loss:  0.736825\n",
      "loss:  0.7386\n",
      "loss:  0.70865\n",
      "loss:  0.62795\n",
      "loss:  0.738475\n",
      "loss:  0.706325\n",
      "loss:  0.6279250000000001\n",
      "loss:  0.596725\n",
      "Degree:  7\n",
      "loss:  0.737125\n",
      "loss:  0.7420749999999999\n",
      "loss:  0.737575\n",
      "loss:  0.723725\n",
      "loss:  0.742525\n",
      "loss:  0.732\n",
      "loss:  0.7352749999999999\n",
      "loss:  0.6652\n",
      "loss:  0.736375\n",
      "loss:  0.717875\n",
      "loss:  0.6829999999999999\n",
      "loss:  0.62775\n",
      "loss:  0.741975\n",
      "loss:  0.6701250000000001\n",
      "loss:  0.627775\n",
      "loss:  0.6221000000000001\n",
      "Best parameters set for ' m_reg_logistic_regression ' using ' accuracy ' was:\n",
      "degree =  7\n",
      "param =  {'max_iters': 20, 'gamma': 0.01, 'lambda_': 0.01, 'initial_w': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1.])}\n",
      "Mean  accuracy  score:  0.742525\n",
      "m_reg_logistic_regression, optimal degree:  7\n",
      "m_logistic_regression, optimal gamma:  0.01\n",
      "m_logistic_regression, optimal lambda_:  0.01\n"
     ]
    }
   ],
   "source": [
    "#Setting tuning\n",
    "regressor = 'm_reg_logistic_regression'\n",
    "compute_loss = 'accuracy'\n",
    "max_iters = 20\n",
    "k_fold = 4\n",
    "gammas = np.logspace(-3, 0, 4)\n",
    "lambdas = np.logspace(-2, 1, 4)\n",
    "params = [{'max_iters': max_iters, 'gamma': gamma, 'lambda_': lambda_} for gamma, lambda_ in product(gammas, lambdas)]\n",
    "degrees = range(8)\n",
    "'''\n",
    "for key in train_dict:\n",
    "    tX_tr = train_dict[key]\n",
    "    tX_va = valid_dict[key] \n",
    "    print(\"Preprocessed data: \", key)\n",
    "'''\n",
    "tX_tr = tX_train\n",
    "tX_va = tX_valid\n",
    "\n",
    "reg_logistic_regression_deg, reg_logistic_regression_param = get_best_degree_params(regressor, compute_loss, y_train, tX_tr, y_valid, tX_va, max_iters, degrees, k_fold, params)\n",
    "print('m_reg_logistic_regression, optimal degree: ', reg_logistic_regression_deg)\n",
    "print('m_logistic_regression, optimal gamma: ', reg_logistic_regression_param['gamma'])\n",
    "print('m_logistic_regression, optimal lambda_: ', reg_logistic_regression_param['lambda_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least squares gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.7752 sec\n",
      "F1-score achieved with 'least_squares_GD': F1 =  0.6657\n",
      "Accuracy achieved with 'least_squares_GD': accuracy =  0.7182\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "initial_w = np.ones(tX_train.shape[1], dtype=float)\n",
    "max_iters = 1300\n",
    "gamma = 0.013\n",
    "\n",
    "# Fitting\n",
    "start = time.time()\n",
    "weights, loss = least_squares_GD(y_train, tX_train, initial_w, max_iters, gamma)\n",
    "exec_time = round(time.time()-start,4)\n",
    "print('Execution time: {} sec'.format(exec_time))\n",
    "\n",
    "# Scoring\n",
    "f1 = f1_score(y_valid, tX_valid, weights)\n",
    "acc = accuracy(y_valid, tX_valid, weights)\n",
    "print(\"F1-score achieved with 'least_squares_GD': F1 = \", f1)\n",
    "print(\"Accuracy achieved with 'least_squares_GD': accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Least squares stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.069 sec\n",
      "F1-score achieved with 'least_squares_SGD': F1 =  0.4584\n",
      "Accuracy achieved with 'least_squares_SGD': accuracy =  0.5984\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "initial_w = np.ones(tX_train.shape[1], dtype=float)\n",
    "max_iters = 9800\n",
    "gamma = 0.098\n",
    "\n",
    "# Fitting\n",
    "start = time.time()\n",
    "weights, loss = least_squares_SGD(y_train, tX_train, initial_w, max_iters, gamma)\n",
    "exec_time = round(time.time()-start,4)\n",
    "print('Execution time: {} sec'.format(exec_time))\n",
    "\n",
    "# Scoring\n",
    "f1 = f1_score(y_valid, tX_valid, weights)\n",
    "acc = accuracy(y_valid, tX_valid, weights)\n",
    "print(\"F1-score achieved with 'least_squares_SGD': F1 = \", f1)\n",
    "print(\"Accuracy achieved with 'least_squares_SGD': accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Least squares (normal equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.005 sec\n",
      "F1-score achieved with 'least_squares': F1 =  0.5537\n",
      "Accuracy achieved with 'least_squares': accuracy =  0.6826\n"
     ]
    }
   ],
   "source": [
    "# Fitting\n",
    "start = time.time()\n",
    "weights, loss = least_squares(y_train, tX_train)\n",
    "exec_time = round(time.time()-start,4)\n",
    "print('Execution time: {} sec'.format(exec_time))\n",
    "\n",
    "# Scoring\n",
    "f1 = f1_score(y_valid, tX_valid, weights)\n",
    "acc = accuracy(y_valid, tX_valid, weights)\n",
    "print(\"F1-score achieved with 'least_squares': F1 = \", f1)\n",
    "print(\"Accuracy achieved with 'least_squares': accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge regression (for least square normal equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.012 sec\n",
      "F1-score achieved with 'ridge_regression': F1 =  0.6654\n",
      "Accuracy achieved with 'ridge_regression': accuracy =  0.7174\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lambda_ = 0.0051\n",
    "\n",
    "# Fitting\n",
    "start = time.time()\n",
    "weights, loss = ridge_regression(y_train, tX_train_binary, lambda_)\n",
    "exec_time = round(time.time()-start,4)\n",
    "print('Execution time: {} sec'.format(exec_time))\n",
    "\n",
    "# Scoring\n",
    "f1 = f1_score(y_valid, tX_valid_binary, weights)\n",
    "acc = accuracy(y_valid, tX_valid_binary, weights)\n",
    "print(\"F1-score achieved with 'ridge_regression': F1 = \", f1)\n",
    "print(\"Accuracy achieved with 'ridge_regression': accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 14.1794 sec\n",
      "F1-score achieved with 'logistic_regression': F1 =  0.5422\n",
      "Accuracy achieved with 'logistic_regression': accuracy =  0.6055\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# TODO: test different values of gamma\n",
    "initial_w = np.ones(tX_train.shape[1], dtype=float)\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "# Fitting\n",
    "start = time.time()\n",
    "weights, loss = logistic_regression(y, tX, initial_w, max_iters, gamma)\n",
    "exec_time = round(time.time()-start, 4)\n",
    "print('Execution time: {} sec'.format(exec_time))\n",
    "\n",
    "# Scoring\n",
    "f1 = f1_score(y_valid, tX_valid, weights)\n",
    "acc = accuracy(y_valid, tX_valid, weights)\n",
    "print(\"F1-score achieved with 'logistic_regression': F1 = \", f1)\n",
    "print(\"Accuracy achieved with 'logistic_regression': accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 14.011 sec\n",
      "F1-score achieved with 'reg_logistic_regression': F1 =  0.5288\n",
      "Accuracy achieved with 'reg_logistic_regression': accuracy =  0.5831\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# TODO: test different values of gamma and lambda\n",
    "lambda_ = 1\n",
    "initial_w = np.ones(tX_train.shape[1], dtype=float)\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "# Fitting\n",
    "start = time.time()\n",
    "weights, loss = reg_logistic_regression(y, tX, lambda_, initial_w, max_iters, gamma)\n",
    "exec_time = round(time.time()-start,4)\n",
    "print('Execution time: {} sec'.format(exec_time))\n",
    "\n",
    "# Scoring\n",
    "f1 = f1_score(y_valid, tX_valid, weights)\n",
    "acc = accuracy(y_valid, tX_valid, weights)\n",
    "print(\"F1-score achieved with 'reg_logistic_regression': F1 = \", f1)\n",
    "print(\"Accuracy achieved with 'reg_logistic_regression': accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: ADD OPTIMIZED REGRESSORS TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "#_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "#create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "3b0b34ccdd8de53604efd8adf53a77b01ed12e4d4bc749635618aa3bfa08aa5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
